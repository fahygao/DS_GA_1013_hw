{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> MATH-GA 2840 HW#1</center></h1>\n",
    "<h3 align=\"right\">Yifei(Fahy) Gao yg1753</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. (Centering) To analyze what happens if we apply PCA without centering, let $\\tilde{x}$ be a $d$ -dimensional vector with mean $\\mu \\in \\mathbb{R}^{d}$ and covariance matrix $\\Sigma_{\\tilde{x}}$ equal to the identity matrix. If we compute the eigendecomposition of the matrix $\\mathrm{E}\\left(\\tilde{x} \\tilde{x}^{T}\\right)$ what is the value of the largest eigenvalue? What is the direction of the corresponding eigenvector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since from the notes: \n",
    "$$\\Sigma\\tilde{x}=\\text{identity matrix}$$\n",
    "\n",
    "Then $$var(\\tilde{x}[i])=1, \\text{and }covar(\\tilde{x}[i],\\tilde{x}[j])=0 \\text{ from regular PCA}$$ \n",
    "\n",
    "Since PCA does subtract mean for centering, so for $\\mathrm{E}\\left(\\tilde{x} \\tilde{x}^{T}\\right)$, we have to get the mean back:  $\\tilde{x}=c(\\tilde{x})+\\mathrm{E}(\\tilde{x})=c(\\tilde{x})+\\mu$\n",
    "\n",
    "Since $\\mathrm{E}\\left(c(\\tilde{x}) c(\\tilde{x})^{T}\\right)=\\mathrm{E}\\left((\\tilde{x}-\\mu) (\\tilde{x}-\\mu)^{T}\\right)=\\mathrm{E}\\left(\\tilde{x}\\tilde{x}^{T}\\right)-\\mathrm{E}\\left(\\tilde{x}\\mu^{T}\\right)-\\mathrm{E}\\left(\\mu\\tilde{x}^{T}\\right)+\\mathrm{E}\\left(\\mu\\mu^{T}\\right)=\\mathrm{E}\\left(\\tilde{x}\\tilde{x}^{T}\\right)-2\\mathrm{E}\\left(\\tilde{x}\\mu^{T}\\right)+\\mathrm{E}\\left(\\tilde{u}\\mu^{T}\\right)=\\mathrm{E}\\left(\\tilde{x}\\tilde{x}^{T}\\right)-\\mu\\mu^T$\n",
    "\n",
    "Then, $\\mathrm{E}\\left(\\tilde{x}\\tilde{x}^{T}\\right)=\\mathrm{E}\\left(c(\\tilde{x}) c(\\tilde{x})^{T}\\right)+\\mu\\mu^T=1+\\mu\\mu^T$\n",
    "\n",
    "In other words, \n",
    "$$\\mathrm{E}\\left(\\tilde{x} \\tilde{x}^{T}\\right)=\\left[\\begin{array}{cccc}\\sigma_{1}^{2}+ \\mu_{1}^2 & covar(\\tilde{x}[1],\\tilde{x}[2])+\\mu_1\\mu_2 & \\cdots &  covar(\\tilde{x}[1],\\tilde{x}[d])+\\mu_{1}\\mu_d \\\\  covar(\\tilde{x}[1],\\tilde{x}[2])+\\mu_1\\mu_2& \\sigma_{2}^{2}+ \\mu_{2}^2 & \\cdots &  covar(\\tilde{x}[2],\\tilde{x}[d])+\\mu_2\\mu_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  covar(\\tilde{x}[1],\\tilde{x}[d])+\\mu_1\\mu_d &  covar(\\tilde{x}[1],\\tilde{x}[2])+\\mu_1\\mu_2 & \\cdots & \\sigma_{d}^{2}+ \\mu_{d}^2\\end{array}\\right]=\\left[\\begin{array}{cccc}1+ \\mu_{1}^2 & 0+\\mu_1\\mu_2 & \\cdots &  0+\\mu_{1}\\mu_d \\\\  0+\\mu_1\\mu_2& 1+ \\mu_{2}^2 & \\cdots & 0+\\mu_2\\mu_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  0+\\mu_1\\mu_d & 0+\\mu_1\\mu_2 & \\cdots & 1+ \\mu_{d}^2\\end{array}\\right]$$\n",
    "\n",
    "And let us do the eigendecomposition of $\\mathrm{E}\\left(\\tilde{x}\\tilde{x}^{T}\\right)$: \n",
    "\n",
    "**Note that we only need to find the eigenvalues of $uu^T$, since both identity matrix and $u u^T$ are both symmetric matrices and the eigenvalue of identity matrix is 1 intuitively.** Then:  \n",
    "\n",
    "let $A=uu^T$\n",
    "$$Au=u u^{T} u=u\\left(u^{T} u\\right)=\\left(u^{T} u\\right) u=\\lambda u$$\n",
    "Then $$\\lambda =u^Tu=\\left\\|\\mathbf\\mu\\right\\|^2$$ and all others are equal to 0 since ùê¥ is an outer-product of ùë¢ on itself.\n",
    "\n",
    "So by spectral theorem the final eigenvalue will be: 1+$\\left\\|\\mathbf\\mu_1\\right\\|^2$ and we can easily see the first $\\mu_1$ points in the direction in which the data has\n",
    "maximum variance which is the $span(\\mu)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. (Low-rank covariance matrix) Let $\\tilde{x}$ be a $d$ -dimensional vector, show that if its covariance matrix $\\Sigma_{\\tilde{x}}$ is rank $r<d,$ then $\\tilde{x}$ is restricted to a hyperplane of dimension $r,$ in the sense that it belongs to the hyperplane with probability one. Recall that by Chebyshev's inequality, for any positive constant $c>0$ and any random variable $\\tilde{a}$ with bounded variance,**\n",
    "$$\n",
    "\\mathrm{P}\\left((\\tilde{a}-\\mathrm{E}(\\tilde{a}))^{2} \\geq c\\right) \\leq \\frac{\\operatorname{Var}(\\tilde{a})}{c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the covariance matrix $\\Sigma_{\\tilde{x}}$ is rank r s.t.  $r<d,$ the dimension of $\\tilde{x}$, then intuitively the eigen-decomposition of this covariance matrix will be: \n",
    "\n",
    "$$A={\\left[\\begin{array}{llll}\n",
    "u_{1} & u_{2} & \\cdots & u_{d}\n",
    "\\end{array}\\right]}\\left[\\begin{array}{cccc}\n",
    "\\lambda_{1} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_{2} & \\cdots & 0 \\\\\n",
    "& & \\cdots &\\lambda_{r} \\\\\n",
    "0 & 0 & \\cdots & 0\\\\\n",
    "0 & 0 & \\cdots & 0\\\\\n",
    "0 & 0 & \\cdots & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{llll}\n",
    "u_{1} & u_{2} & \\cdots & u_{d}\n",
    "\\end{array}\\right]^{T}$$\n",
    "\n",
    "And since $Var(\\tilde{x})=v^T\\tilde{x}$ for any vect v, and a hyperplane through origin can be expressed as $v^T\\tilde{x}=0$ where $c(\\tilde{x})=\\tilde{x}-\\mathrm{E}(\\tilde{x})$for some vectors v and dimension d-1.\n",
    "\n",
    "Then for all the dimention t that $r<t\\leq d$, $$Var(\\tilde{x[i]})=v^T\\tilde{x[i]}=0 \\text{ for i $\\in$ t} $$\n",
    "\n",
    "\n",
    "then $\\tilde{x}$ has bounded variance and by satisfying the requirement of the Chebyshev's inequality, then for all c>0: \n",
    "         $$\n",
    "\\mathrm{P}\\left((\\tilde{x}-\\mathrm{E}(\\tilde{x}))^{2} \\geq c\\right) =\\mathrm{P}\\left((v^T\\tilde{x}-v^T\\mathrm{E}(\\tilde{x})^2 \\geq c\\right) \\leq \\frac{Var(\\tilde{x})}{c}=\\frac{0}{c}=0\n",
    "$$\n",
    "\n",
    "\n",
    "Then $$\\mathrm{P}((v^T\\tilde{x}==v^T\\mathrm{E}(\\tilde{x})^2 )=1$$\n",
    "\n",
    "**Therefore, there are d-r numbers of v, s.t. $v$ are linear independent and the hyperplane characterized as $v^Tc(\\tilde{x})$ have r elements and:** $$kev(v)=r$$**Then we can say that the feasible value of x is a hyperplane of dimention r and $\\tilde{x}$ must be restricted to this hyperplane.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. (Dimensionality reduction) Data containing a large number of features can be difficult to analyze and process. The goal of dimensionality-reduction techniques is to embed the data points in a low-dimensional space where they can be described with a small number of variables. Here we study linear dimensionality reduction, where the lower-dimensional representation is obtained by computing the inner products of each data point with a small number of basis vectors. We will show that PCA provides an optimal choice for the linear transformation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Our proof will be based in the following linear-algebraic result. Let $A \\in \\mathbb{R}^{d \\times d}$ be a symmetric matrix, and $u_{1}, \\ldots, u_{k}$ be its first $k$ eigenvectors. For any set of $k$ orthonormal vectors $v_{1}, \\ldots, v_{k}$\n",
    "$$\n",
    "\\sum_{i=1}^{k} u_{i}^{T} A u_{i} \\geq \\sum_{i=1}^{k} v_{i}^{T} A v_{i}\n",
    "$$\n",
    "Show that this follows from the spectral theorem using induction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider the case when k is equal to 1, then the left hand of the inequality will be $\n",
    "u_{1}^{T} A u_{1} \n",
    "$. \n",
    "\n",
    "Since A is a symmetric matrix so all eigenvalues and eigenvectors are all real, then by spectral theorem: \n",
    "$$\n",
    "A={\\left[\\begin{array}{llll}\n",
    "u_{1} & u_{2} & \\cdots & u_{d}\n",
    "\\end{array}\\right]}\\left[\\begin{array}{cccc}\n",
    "\\lambda_{1} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_{2} & \\cdots & 0 \\\\\n",
    "& & \\cdots & \\\\\n",
    "0 & 0 & \\cdots & \\lambda_{d}\n",
    "\\end{array}\\right]\\left[\\begin{array}{llll}\n",
    "u_{1} & u_{2} & \\cdots & u_{d}\n",
    "\\end{array}\\right]^{T}\n",
    "$$\n",
    "\n",
    "\n",
    "since by spectral theorem$\n",
    "\\lambda_1=\\max x^T A x $ and $u_{1}=\\arg \\max _{\\|x\\|_{2}=1} x^{T} A x\n",
    "$, then \n",
    "\n",
    "$\\lambda_1 x^T=\\max x^T A$ and $\\lambda_1 u^T=\\max u^T A$\n",
    "\n",
    "Then, $u_{1}^{T} A u_{1}=\\lambda_1 u_{1}^Tu_{1}=\\lambda_1\\geq v_{1}^{T} A v_{1}$ where v is any set of k orthonormal vectors $v_1,...,v_k$, since $\\lambda_1$ is the $ \\max Var(x)=v^Tx $ for any vector $v$.\n",
    "\n",
    "Let us assume the $\\sum_{i=1}^{k} u_{i}^{T} A u_{i} \\geq \\sum_{i=1}^{k} v_{i}^{T} A v_{i}$ is true, \n",
    "\n",
    "then for k+1: \n",
    "\n",
    "$$\\sum_{i=1}^{k+1} u_{i}^{T} A u_{i}= \\sum_{i=1}^{k} u_{i}^{T} A u_{i}+ u_{k+1}^{T} A u_{k+1}$$\n",
    "\n",
    "Except $u_1$, the largest variance will be $\\lambda_2$ because $$\n",
    "\\lambda_{k}=\\max _{\\|x\\|_{2}=1, x \\perp u_{1}, \\ldots, u_{k-1}} x^{T} A x, \\quad 2 \\leq k \\leq d\n",
    "\\tag{1}$$by spectral theorem. \n",
    "\n",
    "Then same for $\\lambda_{k+1}$ as in: \n",
    "\n",
    "Since $\\sum_{i=1}^{k} u_{i}^{T} A u_{i}=\\sum_{i=1}^{k} \\lambda_i u^Tu=\\sum_{i=1}^{k} \\lambda_i$, \n",
    "\n",
    "Then:\n",
    "$$\\sum_{i=1}^{k} u_{i}^{T} A u_{i}+ u_{k+1}^{T} A u_{k+1}=\\lambda_{1}+\\lambda_{2}+\\cdots+\\lambda_{k}+\\lambda_{k+1}\\geq v_{1}^{T}Av_{1}+\\cdots+v_{k+1}^{T}Av_{k+1}=\\sum_{i=1}^{k+1} v_{i}^{T} A v_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Let us consider an arbitrary dataset $X=\\left\\{x_{1}, \\ldots, x_{n}\\right\\}$ of $d$ -dimensional points. For a fixed dimension $k<d,$ and any set of $k$ orthonormal vectors $v_{1}, \\ldots, v_{k}$ show that the sum of the sample variances of the $k$ first principal components,\n",
    "$$\n",
    "\\sum_{i=1}^{k} \\operatorname{var}(\\mathrm{pc}[i]) \\geq \\sum_{i=1}^{k} \\sigma_{X v_{i}}^{2}\n",
    "$$\n",
    "where $\\sigma_{X_{v_{i}}}^{2}$ is the sample variance of $\\left\\{v_{i}^{T} x_{1}, \\ldots, v_{i}^{T} x_{n}\\right\\} .$ This establishes that projecting onto the principal directions is the linear transformation that preserves the most variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since from the notes (54-56): $$\\begin{aligned} \\operatorname{Var}({p c}[i]) &=u_{i}^{T} \\Sigma_{\\tilde{x}} u_{i} \\\\ &=\\lambda_{i} u_{i}^{T} u_{i} \\\\ &=\\lambda_{i} \\end{aligned}$$\n",
    "\n",
    "Then $\\sum_{i=1}^{k}\\operatorname{Var}({p c}[i])=\\sum_{i=1}^{k}\\lambda_{i}$ and we know from note (30) that $\\sigma^2_{X_{v}}=v^T\\Sigma_{X}v$ as sample variance, then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the inequality will be $$\\sum_{i=1}^{k}\\lambda_{i}\\geq\\sum_{i=1}^{k}v_{i}^T\\Sigma_{X}v_{i}$$\n",
    "\n",
    "\n",
    "\n",
    "For k=1,\n",
    "$$\\lambda_1\\geq v^TXv=Var(v^TX) \\text{ for any vector v in $R^d$}$$\n",
    "Then $\\text{for }\\sum_{i=1}^{k}\\lambda_1\\geq\\sum_{i=1}^{k} v^TAv \\text{ for any vector v in $R^d$ is true}$,\n",
    "\n",
    "By (1) in part a, then we know $\\lambda_k+1\\geq v_{k+1}^TXv_{k+1}$ then: $$ \\sum_{i=1}^{k+1}\\lambda_1\\geq\\sum_{i=1}^{k+1} v^TAv$$\n",
    "\n",
    "Therefore, $$\\sum_{i=1}^{k} \\operatorname{var}(\\mathrm{pc}[i])=\\sum_{i=1}^{k}\\lambda_{i}\\geq\\sum_{i=1}^{k}v_{i}^T\\Sigma_{X}v_{i}=\\sum_{i=1}^{k} \\sigma_{X v_{i}}^{2}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
