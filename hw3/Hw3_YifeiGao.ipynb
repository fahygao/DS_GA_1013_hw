{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> MATH-GA 2840 HW#3</center></h1>\n",
    "<h3 align=\"right\">Yifei(Fahy) Gao yg1753</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (Augmented dataset) Ridge regression is equivalent to applying OLS on an expanded dataset that has additional examples. Describe these additional examples in detail. Intuitively, what effect do these additional examples have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS cost function is: $$\\arg \\min _{\\beta}\\left\\|\\tilde{y}_{\\text {train }}-X^{T} \\beta\\right\\|_{2}^{2}=\\arg \\min _{\\beta}\\left(\\beta-\\beta_{\\text {true }}\\right)^{T} X X^{T}\\left(\\beta-\\beta_{\\text {true }}\\right)-2 \\tilde{z}_{\\text {train }}^{T} X^{T} \\beta$$\n",
    "\n",
    "Ridge regression cost function is: $$\\arg \\min _{\\beta}\\left\\|\\tilde{y}_{\\text {train }}-X^{T} \\beta\\right\\|_{2}^{2}+\\lambda\\left\\| \\beta\\right\\|_{2}^{2}=\\arg \\min _{\\beta}\\left(\\beta-\\beta_{\\text {true }}\\right)^{T} X X^{T}\\left(\\beta-\\beta_{\\text {true }}\\right)+\\lambda\\beta^T\\beta-2 \\tilde{z}_{\\text {train }}^{T} X^{T} \\beta$$\n",
    "\n",
    "So the ridge has the extra $\\lambda\\beta^T\\beta$ term comparing to the OLS cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the quadratic form of OLS is (let $\\beta=\\begin{bmatrix}\n",
    "a \\\\\n",
    "b\n",
    "\\end{bmatrix}, \\beta_{true}=\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}, X=\\begin{bmatrix}\n",
    "1&0 \\\\\n",
    "0&0.01\n",
    "\\end{bmatrix}$), then: $$c^2=\\arg \\min _{\\beta}\\left(\\beta-\\beta_{\\text {true }}\\right)^{T} X X^{T}\\left(\\beta-\\beta_{\\text {true }}\\right)\\\\=\\begin{bmatrix}\n",
    "a-1 \\\\\n",
    "b-1\n",
    "\\end{bmatrix}^T\\begin{bmatrix}\n",
    "1&0 \\\\\n",
    "0&0.01\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "a-1 \\\\\n",
    "b-1\n",
    "\\end{bmatrix}=(a-1)^2+\\frac{(b-1)^2}{100}$$\n",
    "\n",
    "Then the minima for OLS is: \n",
    "$$\\begin{aligned}\n",
    "\\beta_{\\mathrm{OLS}} &=\\left(X X^{T}\\right)^{-1} X \\tilde{y}_{\\text {train }} \\\\\n",
    "&=\\beta_{\\text {true }}+U S^{-1} V^{T} \\tilde{z}_{\\text {train }}\n",
    "\\end{aligned}$$\n",
    "So it is Gaussian with the mean equal: $\\beta_{true}=\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}$\n",
    "And for the quadratic form of Ridge regression: \n",
    "\n",
    "$$c^2=\\arg \\min _{\\beta}\\left(\\beta-\\beta_{\\text {true }}\\right)^{T} X X^{T}\\left(\\beta-\\beta_{\\text {true }}\\right)+\\lambda\\beta^T\\beta\\\\\n",
    "=\\begin{bmatrix}\n",
    "a-1 \\\\\n",
    "b-1\n",
    "\\end{bmatrix}^T\\begin{bmatrix}\n",
    "1&0 \\\\\n",
    "0&0.01\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "a-1 \\\\\n",
    "b-1\n",
    "\\end{bmatrix}+\\lambda a^2+\\lambda b^2$$\n",
    "(noted that $\\lambda a^2+\\lambda b^2$ is a spherical equation)\n",
    "\n",
    "continue the simplication: \n",
    "$$\n",
    "f(c,\\lambda)=(1+\\lambda)(a-\\frac{1}{1+\\lambda})^2+\\frac{1+100\\lambda}{100}(b-\\frac{1}{1+100\\lambda})^2\n",
    "$$\n",
    "\n",
    "\n",
    "So when $\\lambda$ closes to 0, $f(c,\\lambda)=\\beta_{OLS}$(ellipse), when $\\lambda$ close to $\\infty$, $f(c,\\lambda)=0$ (circle).\n",
    "\n",
    "And the minima for Ridge Regression is: \n",
    "\n",
    "$$\n",
    "\\beta_{\\mathrm{RR}} =\\left(X X^{T}+\\lambda I\\right)^{-1} X\\left(X^{T} \\beta_{\\text {true }}+\\tilde{z}_{\\text {train }}\\right)=\\left[\\begin{array}{c}\n",
    "\\frac{1}{1+\\lambda} \\\\\n",
    "\\frac{0.01}{0.01+\\lambda}\n",
    "\\end{array}\\right]+\\left[\\begin{array}{cc}\n",
    "\\frac{1}{1+\\lambda} & 0 \\\\\n",
    "0 & \\frac{1}{0.01+\\lambda}\n",
    "\\end{array}\\right] X \\tilde{z}_{\\text {train }}\n",
    "$$\n",
    "So it is Gaussian with the mean equal $\\begin{bmatrix}\n",
    "\\frac{1}{1+\\lambda} \\\\\n",
    "\\frac{0.01}{0.01+\\lambda}\n",
    "\\end{bmatrix}$, which is much less than the mean of OLS.\n",
    "\n",
    "\n",
    "The effect of adding the extra term is to do regulation with much less variance and to avoid the overfitting situation that we do not have enough data but too many noises comparing to OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (Correlated features) Consider a regression problem where the response only depends on one feature, but we don't know it, so we incorporate an additional feature into the model that happens to be very correlated with the first feature. More specifically, let $y \\in \\mathbb{R}^{n}$ be defined by\n",
    "$$\n",
    "y:=\\beta_{\\text {true }} w_{1}+z\n",
    "$$\n",
    "where $\\beta_{\\text {true }} \\in \\mathbb{R}$ is the true coefficient, $w_{1} \\in \\mathbb{R}^{n}$ is the first feature vector, and $z \\in \\mathbb{R}^{n}$ is additive noise. The second feature vector is given by $w_{2} \\in \\mathbb{R}^{n}$ and can be decomposed into\n",
    "$$\n",
    "w_{2}=\\alpha w_{1}+\\sqrt{1-\\alpha^{2}} w_{\\perp}\n",
    "$$\n",
    "where $w_{\\perp}$ is orthogonal to $w_{1}$. The vectors $w_{1}, w_{2}, w_{\\perp}$ and $z$ all have unit $\\ell_{2}$ norm. In addition, we assume\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "w_{1}^{T} z=0.1 \\\\\n",
    "w_{\\perp}^{T} z=0.1\n",
    "\\end{array}\n",
    "$$\n",
    "We fit a linear regression model to $y$ using the feature matrix\n",
    "$$\n",
    "X=\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "(a) What does the OLS estimator of the coefficients $\\beta_{\\text {OLS }}$ equal to when $\\alpha \\rightarrow 1$ ? Explain what is happening. Hint: Use the fact that for any $a, b, c,$ and $d$ such that $a d \\neq b c$\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{array}\\right]^{-1}=\\frac{1}{a d-b c}\\left[\\begin{array}{cc}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know $$\\begin{aligned}\n",
    "\\beta_{\\mathrm{OLS}} &:=\\arg \\min _{\\beta} \\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{T} \\beta\\right)^{2} \\\\\n",
    "&=\\left(X X^{T}\\right)^{-1} X y\n",
    "\\end{aligned}\\tag{Theorem 2.1}$$\n",
    "\n",
    "Then: $$\\begin{array}{l}\\beta_{\\mathrm{OLS}} =(\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]^T)^{-1}\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]y\\\\ =\\left[\\begin{array}{l}\n",
    "w_{1}^{T}w_{1}& w_{1}^{T}w_{2}\\\\\n",
    "w_{2}^{T}w_{1}& w_{2}^{T}w_{2}\n",
    "\\end{array}\\right]^{-1}\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]y\\end{array}$$\n",
    "\n",
    "from the hint, we get: \n",
    "$$\\beta_{\\mathrm{OLS}}=\\left[\\begin{array}{l}\n",
    "1& w_{1}^{T}w_{2}\\\\\n",
    "w_{2}^{T}w_{1}& 1\n",
    "\\end{array}\\right]^{-1}\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]y=\\frac{1}{1-w_{1}^Tw_{2}w_{2}^{T}w_{1}}\\left[\\begin{array}{l}\n",
    "1& -w_{1}^{T}w_{2}\\\\\n",
    "-w_{2}^{T}w_{1}& 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]y$$\n",
    "\n",
    "Since $w_{2}=\\alpha w_{1}+\\sqrt{1-\\alpha^2}w_{\\perp}$, $y:=\\beta_{true}w_1+z$, $\\begin{array}{l}\n",
    "w_{1}^{T} z=0.1 & w_{\\perp}^{T} z=0.1\n",
    "\\end{array}$and $w_{\\perp}w_1^T=w_{\\perp}^Tw_1=0$, \n",
    "\n",
    "$\\beta_{\\mathrm{OLS}}=\\frac{1}{1-w_{1}^T(\\alpha w_{1}+\\sqrt{1-\\alpha^2}w_{\\perp})(\\alpha w_{1}+\\sqrt{1-\\alpha^2}w_{\\perp})^{T}w_{1}}\\left[\\begin{array}{l}\n",
    "1& -w_{1}^{T}(\\alpha w_{1}+\\sqrt{1-\\alpha^2}w_{\\perp})\\\\\n",
    "-(\\alpha w_{1}+\\sqrt{1-\\alpha^2}w_{\\perp})^{T}w_{1}& 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "(\\alpha w_{1}+\\sqrt{1-\\alpha^2}w_{\\perp})^{T}\n",
    "\\end{array}\\right](\\beta_{true}w_1+z)$\n",
    "$$\n",
    "=\\frac{1}{1-\\alpha^2}\\left[\\begin{array}{l}\n",
    "1&-\\alpha \\\\\n",
    "-\\alpha&1\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "(\\alpha w_{1}+\\sqrt{1-\\alpha^2}w_{\\perp})^{T}\n",
    "\\end{array}\\right](\\beta_{true}w_1+z)\\\\=\\frac{1}{1-\\alpha^2}\\left[\\begin{array}{l}\n",
    "w_{1}^{T}-\\alpha(\\alpha w_{1}^{T}+\\sqrt{1-\\alpha^2}w_{\\perp}) \\\\\n",
    "-\\alpha w_1^T+\\alpha w_1^T+\\sqrt{1-\\alpha^2}w_{\\perp}\n",
    "\\end{array}\\right](\\beta_{true}w_1+z)\\\\\n",
    "=\\frac{1}{1-\\alpha^2}\\left[\\begin{array}{l}\n",
    "\\beta_{true}+0.1-\\alpha^2\\beta_{true}-0.1\\alpha^2-\\beta_{true}\\sqrt{1-\\alpha^2}w_{\\perp}^{T}w_1-0.1\\alpha \\sqrt{1-\\alpha^2} \\\\\n",
    "\\beta_{true}\\sqrt{1-\\alpha^2}w_{\\perp}^{T}w_1+\\sqrt{1-\\alpha^2}w_{\\perp}^{T}z\n",
    "\\end{array}\\right]\\\\\n",
    "=\\frac{1}{1-\\alpha^2}\\left[\\begin{array}{l}\n",
    "(1-\\alpha^2)\\beta_{true}+0.1(1-\\alpha^2)-0.1\\alpha\\sqrt{1-\\alpha^2}\\\\\n",
    "0.1\\sqrt{1-\\alpha^2}\n",
    "\\end{array}\\right]\\\\\n",
    "\\beta_{OLS}=\\left[\\begin{array}{l}\n",
    "\\beta_{true}+0.1-\\frac{0.1\\alpha}{\\sqrt{1-\\alpha^2}}\\\\\n",
    "\\frac{0.1}{\\sqrt{1-\\alpha^2}}\n",
    "\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore when a is approaching to a, the $\\beta_{OLS}$ will be infinite, becuase the denominator of $\\frac{0.1}{\\sqrt{1-\\alpha^2}}$will be very close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(b) What does the corresponding estimate of the response $y_{\\mathrm{OLS}}:=X^{T} \\beta_{\\mathrm{OLS}}$ equal to when $\\alpha \\rightarrow 1 ?$ Is it collinear with the true feature $w_{1}$ when $\\alpha \\rightarrow 1 ?$ Explain what is happening.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_{\\mathrm{OLS}}:=X^{T} \\beta_{\\mathrm{OLS}}=\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]^{T}\\beta_{OLS}=\\left[\\begin{array}{l}\n",
    "w_{1}& w_{2}\n",
    "\\end{array}\\right]\\beta_{OLS}\\\\\n",
    "=\\left[\\begin{array}{l}\n",
    "w_{1}& \\alpha w_{1}+\\sqrt{1-\\alpha^{2}} w_{\\perp}\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "\\beta_{true}+0.1-\\frac{0.1\\alpha}{\\sqrt{1-\\alpha^2}}\\\\\n",
    "\\frac{0.1}{\\sqrt{1-\\alpha^2}}\n",
    "\\end{array}\\right]\\\\\n",
    "=\\beta_{true} w_1+0.1w_1-\\frac{0.1\\alpha w_1}{\\sqrt{1-\\alpha^2}}+\\frac{0.1\\alpha w_1}{\\sqrt{1-\\alpha^2}}+0.1w_{\\perp}\\\\\n",
    "=\\beta_{true}  w_1+0.1(w_1+w_{\\perp})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore $y_{OLS}$ is not collinear with the feature $w_1$ when $\\alpha \\rightarrow 1$, since there exits $w_{\\perp}$ in the expression of $y_{OLS}$. The reason is that there are some noise in the same direction of $w_{\\perp}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(c) What does the ridge regression estimator of the coefficients $\\beta_{\\mathrm{RR}}$ equal to when $\\alpha \\rightarrow 1$ and the regularization parameter $\\lambda>0$ is fixed? Describe the difference with the OLS estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the *regulation lecture notes*: \n",
    "$$\\beta_{\\mathrm{RR}}=\\left(X X^{T}+\\lambda I\\right)^{-1} X y \\tag{Theorem 1.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have already calculate $XX^T$ from the part a, then:\n",
    "$$\\beta_{RR}=\\left[\\begin{array}{l}\n",
    "1+\\lambda& w_{1}^{T}w_{2}\\\\\n",
    "w_{2}^{T}w_{1}& 1+\\lambda\n",
    "\\end{array}\\right]^{-1}\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]y\\\\\n",
    "=\\left[\\begin{array}{l}\n",
    "1+\\lambda& \\alpha\\\\\n",
    "\\alpha& 1+\\lambda\n",
    "\\end{array}\\right]^{-1}\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "(\\alpha w_{1}+\\sqrt{1-\\alpha^{2}} w_{\\perp})^{T}\n",
    "\\end{array}\\right](\\beta_{\\text {true }} w_{1}+z)\\\\\n",
    "=\\frac{1}{1+\\lambda^2+2\\lambda-\\alpha^2}\\left[\\begin{array}{l}\n",
    "1+\\lambda& -\\alpha\\\\\n",
    "-\\alpha& 1+\\lambda\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "(\\alpha w_{1}+\\sqrt{1-\\alpha^{2}} w_{\\perp})^{T}\n",
    "\\end{array}\\right](\\beta_{\\text {true }} w_{1}+z)\\\\\n",
    "=\\frac{1}{(1+\\lambda)^{2}-\\alpha^{2}}\\left[\\begin{array}{cc}\n",
    "1+\\lambda & -\\alpha \\\\\n",
    "-\\alpha & 1+\\lambda\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{\\text {true }}+0.1 \\\\\n",
    "\\alpha \\beta_{\\text {true }}+0.1\\left(\\alpha+\\sqrt{1-\\alpha^{2}}\\right)\n",
    "\\end{array}\\right]\\\\\n",
    "=\\frac{1}{(1+\\lambda)^{2}-\\alpha^{2}}\\left[\\begin{array}{c}\n",
    "(1+\\lambda)(\\beta_{\\text {true }}+0.1)-\\alpha (\\alpha \\beta_{\\text {true }}+0.1\\left(\\alpha+\\sqrt{1-\\alpha^{2}}\\right)) \\\\\n",
    "-\\alpha(\\beta_{\\text {true }}+0.1)+(1+\\lambda)(\\alpha \\beta_{\\text {true }}+0.1\\left(\\alpha+\\sqrt{1-\\alpha^{2}}\\right))\n",
    "\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if a is close to 1, then the equation will be: \n",
    "$$=\\frac{1}{(1+\\lambda)^{2}-1}\\left[\\begin{array}{c}\n",
    "(1+\\lambda)(\\beta_{\\text {true }}+0.1)-(\\beta_{\\text {true }}+0.1\\left(1+\\sqrt{1-1}\\right)) \\\\\n",
    "-(\\beta_{\\text {true }}+0.1)+(1+\\lambda)( \\beta_{\\text {true }}+0.1\\left(1+\\sqrt{1-1}\\right))\n",
    "\\end{array}\\right]\\\\\n",
    "=\\frac{1}{\\lambda^2+2\\lambda}\\left[\\begin{array}{c}\n",
    "\\lambda(\\beta_{\\text {true }}+0.1) \\\\\n",
    "\\lambda(\\beta_{\\text {true }}+0.1)\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\frac{\\beta_{\\text {true }}+0.1}{\\lambda+2} \\\\\n",
    "\\frac{\\beta_{\\text {true }}+0.1}{{\\lambda+2}}\n",
    "\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the part a), we can see that $\\beta_{RR}$ is independent from $\\alpha$ but $\\lambda$, and the front part of $\\beta_{RR}$:$\\left(X X^{T}+\\lambda I\\right)^{-1}$\n",
    "is always invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(d) What does the corresponding estimate of the response $y_{\\mathrm{RR}}:=X^{T} \\beta_{\\mathrm{RR}}$ equal to when $\\alpha \\rightarrow 1 ?$ Is it collinear with the true feature $w_{1} ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_{\\mathrm{RR}}:=X^{T} \\beta_{\\mathrm{RR}}=\\left[\\begin{array}{l}\n",
    "w_{1}^{T} \\\\\n",
    "w_{2}^{T}\n",
    "\\end{array}\\right]^{T}\\beta_{RR}=\\left[\\begin{array}{l}\n",
    "w_{1}& w_{2}\n",
    "\\end{array}\\right]\\beta_{RR}\\\\\n",
    "=\\left[\\begin{array}{l}\n",
    "w_{1}& \\alpha w_{1}+\\sqrt{1-\\alpha^{2}} w_{\\perp}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\frac{\\beta_{\\text {true }}+0.1}{\\lambda+2} \\\\\n",
    "\\frac{\\beta_{\\text {true }}+0.1}{{\\lambda+2}}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "As a close to 1, \n",
    "\n",
    "$$\\lim _{\\alpha \\rightarrow 1} y_{\\mathrm{RR}}= \\lim _{\\alpha \\rightarrow 1}(\\left[\\begin{array}{l}\n",
    "w_{1}& \\alpha w_{1}+\\sqrt{1-\\alpha^{2}} w_{\\perp}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\frac{\\beta_{\\text {true }}+0.1}{\\lambda+2} \\\\\n",
    "\\frac{\\beta_{\\text {true }}+0.1}{{\\lambda+2}}\n",
    "\\end{array}\\right])$$\n",
    "$$\n",
    "=\\left[\\begin{array}{l}\n",
    "w_{1}& w_{1}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\frac{\\beta_{\\text {true }}+0.1}{\\lambda+2} \\\\\n",
    "\\frac{\\beta_{\\text {true }}+0.1}{{\\lambda+2}}\n",
    "\\end{array}\\right]\\tag{product rule of limit}$$$$\n",
    "=\\frac{2}{\\lambda+2}(\\beta_{\\text {true }}+0.1)w_{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can say that $y_{RR}$ is collinear with the true feature $w_1$, since it only contains $w_1$ unlike $y_{OLS}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (Prior knowledge) Consider a linear regression problem where we have prior information indicating that the coefficients should be close to a certain value $\\beta_{\\text {prior }}$.\n",
    "\n",
    "(a) How can you incorporate this prior knowledge if you are using ridge regression? Write the corresponding optimization problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the *regularization lecture notes*, \n",
    "the Ridge Regression estimator is $$\\beta_{\\mathrm{RR}}:=\\arg \\min _{\\beta}\\left\\|y-X^{T} \\beta\\right\\|_{2}^{2}+\\lambda\\|\\beta\\|_{2}^{2}\\tag{Definition 1.1}$$\n",
    "We can try to incorporate the $\\beta_{prior}$ into the $\\lambda$ norm term, so that: \n",
    "\n",
    "$$\\beta_{RR}=\\arg \\min _{\\beta}\\left\\|y-X^{T} \\beta\\right\\|_{2}^{2}+\\lambda\\|\\beta-\\beta_{prior}\\|_{2}^{2}$$\n",
    "\n",
    "By decomposition, we get: \n",
    "$$\\beta_{RR}=y^{T}y-2X^T\\beta y+X^{T}\\beta \\beta^{T}X+\\lambda\\beta^T\\beta-2\\lambda\\beta \\beta_{prior}+\\lambda \\beta_{prior}^T\\beta_{prior}$$\n",
    "\n",
    "Then differentiate the whole equation by $\\beta$ and let $\\frac{\\partial\\beta_{RR}}{\\partial\\beta} =0$, then\n",
    "\n",
    "$$\\nabla f(\\beta)=2X^{T}X \\beta-2X^{T}y+2\\lambda \\beta-2\\lambda \\beta_{\\text {prior }}$$\n",
    "$$0=-2X^T y+2X^{T}X\\beta+2\\lambda\\beta-2\\lambda \\beta_{prior}$$\n",
    "$$0=(-X^{T}X-\\lambda)\\beta+X^T y+\\lambda \\beta_{prior}$$\n",
    "$$\\beta_{RR}=(XX^{T}+\\lambda)^{-1}(X y+\\lambda \\beta_{prior})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Assume that the data are generated according to a linear model $\\tilde{y}:=X^{T} \\beta_{\\text {true }}+\\tilde{z}$, where $\\beta_{\\text {true }} \\in \\mathbb{R}^{p}$ and $X \\in \\mathbb{R}^{p \\times n}$ are fixed and $\\tilde{z}$ is an iid Gaussian random vector with zero mean and variance $\\sigma^{2} .$ Does the modification change the mean or the covariance matrix of the coefficient estimate with respect to the ridge-regression estimate? If so, report the new value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean will change but the covariance matrix will not. \n",
    "\n",
    "By Singular-value-decomposition, we can let $X=USV^T$ to be the SVD of the feature matrix, then:\n",
    "\n",
    "$$\\beta_{RR}=(X^{T}X+\\lambda)^{-1}(X y+\\lambda \\beta_{prior})\\\\\n",
    "=(XX^{T}+\\lambda)^{-1}(X(X^{T} \\beta_{\\text {true }}+\\tilde{z})+\\lambda \\beta_{prior})\\\\\n",
    "=(US^2U^{T}+\\lambda)^{-1}(US^2U^{T} \\beta_{\\text {true }}+USV^T\\tilde{z}+\\lambda \\beta_{prior})\\\\\n",
    "=(U(S^2+\\lambda)^{-1}U^{T})(US^2U^{T} \\beta_{\\text {true }}+USV^T\\tilde{z}+\\lambda \\beta_{prior})\\\\\n",
    "\\begin{array}{l}\n",
    "=U\\left(S^{2}+\\lambda \\mathrm{Id}_{\\mathrm{p}}\\right)^{-1} S^{2} U^{T} \\beta_{\\text {true }}+U\\left(S^{2}+\\lambda \\mathrm{Id}_{\\mathrm{p}}\\right)^{-1} S V^{T} \\tilde{z}+U\\left(S^{2}+\\lambda \\mathrm{Id}_{\\mathrm{p}}\\right)^{-1} U^{T} \\lambda \\beta_{\\text {prior }}\n",
    "\\end{array}$$\n",
    "\n",
    "Therefore, $$E[\\beta_{RR}]=\\sum_{j=1}^{p} \\frac{s_{j}^{2}\\left\\langle u_{j}, \\beta_{\\text {true }}\\right\\rangle}{s_{j}^{2}+\\lambda} u_{j}+\\lambda\\sum_{j=1}^{p} \\frac{\\left\\langle u_{j}, \\beta_{\\text {prior }}\\right\\rangle}{s_{j}^{2}+\\lambda} u_{j}$$\n",
    "\n",
    "And the original one is: $$E[\\beta_{OLS}]=\\sum_{j=1}^{p} \\frac{s_{j}^{2}\\left\\langle u_{j}, \\beta_{\\text {true }}\\right\\rangle}{s_{j}^{2}+\\lambda} u_{j}$$\n",
    "And the covariance will not change, which is still the same as: $$\\left.\\operatorname{Cov}\\left(\\beta_{\\mathrm{RR}}\\right)=\\left(X X^{T}+\\lambda\\right)_{1}^{-1} X \\sigma^{2}\\left((X X^{T}+\\lambda\\right)^{-1} X\\right)^{T}=\\sigma^{2} U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{s_{j}^{2}}{\\left(s_{j}^{2}+\\lambda\\right)^{2}}\\right) U^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(c) How can you incorporate this prior knowledge if you are using gradient descent with early stopping? Write the corresponding update equation as a function of $\\beta_{\\text {prior }}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let new $\\nabla f(\\beta)$equal to : $X^{T}X \\beta-X^{T}y+\\lambda \\beta-\\lambda \\beta_{\\text {prior }}$\n",
    "\n",
    "Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta^{k+1}=\\beta^{k}-\\alpha(X^{T}X\\beta^{k}-Xy+\\lambda\\beta^{k}-\\lambda \\beta_{prior})$$\n",
    "$$\\beta^{k+1}=(I-\\alpha X^TX-\\alpha\\lambda)\\beta^{k}+\\alpha(Xy+\\lambda \\beta_{prior})$$\n",
    "since $\\beta^{0}=\\beta_{prior}$\n",
    "$$\\beta^{(k+1)}=\\left(I-\\alpha X X^{T}\\right)^{k+1} \\beta_{p r i o r}+\\sum_{i=0}^{k}\\left(I-\\alpha X X^{T}\\right)^{i} \\alpha (Xy+\\lambda \\beta_{prior})\\tag{Theorem 2.1}$$\n",
    "$$\\beta^{k+1}=\\sum_{i=0}^{k}\\left(I-\\alpha X X^{T}\\right)^{i} \\alpha\\left(X y+\\lambda \\beta_{\\text {prior }}\\right)\\tag{from the early stopping video}$$\n",
    "\n",
    "Since from the part b, $X=USV^T, I= UU^T$,\n",
    "$$\\begin{array}{l}\n",
    "\\beta^{k+1}=\\sum_{i=0}^{k}\\left(U U^{T}-\\alpha U S^{2} U^{T}\\right)^{i}\\alpha \\left(U S V^{T} y+\\lambda \\beta_{\\text {prior }}\\right) \\\\\n",
    "\\beta^{k+1}=\\alpha \\sum_{i=0}^{k}\\left(U\\left(I-\\alpha S^{2}\\right) U^{T}\\right)^{i}\\left(U S V^{T} y+\\lambda \\beta_{\\text {prior }}\\right) \\\\\n",
    "\\beta^{k+1}=\\alpha \\sum_{i=0}^{k} U\\left(I-\\alpha S^{2}\\right)^{i}\\left(S V^{T} y+U^{T} \\lambda \\beta_{\\text {prior }}\\right) \\\\\n",
    "\\beta^{k+1}=\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)\\left(S V^{T} y+U^{T} \\lambda \\beta_{\\text {prior }}\\right)\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Assume that the data are generated according to the linear model described above. Does the modification change the mean or the covariance matrix of the coefficient estimate with respect to the gradient descent estimate initialized at the origin? If so, report the new value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same the as the question (b), the mean will change bu the covariance matrix will not.\n",
    "\n",
    "$$E[\\beta^{k+1}]=E[\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)\\left(S V^{T} +U^{T} \\lambda \\beta_{\\text {prior }}\\right)]$$\n",
    "Since $y=X^T\\beta_{true}+\\tilde{z}$, then \n",
    "$$E[\\beta^{k+1}]=E[\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)\\left(S V^{T}(X^T\\beta_{true}+\\tilde{z})+U^{T} \\lambda \\beta_{\\text {prior }}\\right)]\\\\\n",
    "=E[\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)\\left(S V^{T}X^T\\beta_{true}+S V^{T}\\tilde{z}+U^{T} \\lambda \\beta_{\\text {prior }}\\right)]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By linearity of the expectation value,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "E[\\beta^{k+1}]=E[\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)(S V^{T}X^T\\beta_{true})]\\\\+E[\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)(S V^{T}\\tilde{z})]\\\\+E[\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)(U^{T} \\lambda \\beta_{\\text {prior }})\\\\\n",
    "E[\\beta^{k+1}]=E[\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)(S V^{T}X^T\\beta_{true})]+E[\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)(U^{T} \\lambda \\beta_{\\text {prior }})\\\\\n",
    "E[\\beta^{k+1}]=\\alpha U \\operatorname{diag}_{j=1}^{p}\\left(\\frac{1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}}{\\alpha s_{j}^{2}}\\right)\\left(S^{2} U^{T} \\beta_{\\text {true }}+U^{T} \\lambda \\beta_{\\text {prior }}\\right)\\\\\n",
    "E[\\beta^{k+1}]=\\left.U \\operatorname{diag}_{j=1}^{p}\\left(1-\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}\\right) U^{T} \\beta_{\\text {true }}+U \\operatorname{diag}_{j=1}^{p}\\left(\\left(1-\\alpha s_{j}^{2}\\right)^{k+1}\\right) U^{T} \\beta_{\\text {prior }}\\right)\n",
    "\\end{aligned}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the covariance will not change since the extra term we added do not affect the noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (Stochastic Gradient Descent) In class we saw how to use gradient descent to solve an optimization problem and applied it to ordinary least squares and ridge regression. However, gradient descent is often not used in practice. When the training data set is very large, computing the gradient of the objective function can take a long time as we need to go through the whole dataset for a single gradient step. When the objective function takes the form of an average of many values, such as in the case of linear regression\n",
    "$$\n",
    "J(\\beta)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(\\beta^{T} x_{i}-y_{i}\\right)^{2}\n",
    "$$\n",
    "stochastic gradient descent (SGD) can be very effective.\n",
    "\n",
    "(a) Here In SGD, rather than taking $-\\nabla J(\\beta)$ as our step direction (as in gradient descent), we take the gradient of objective function assuming there there is only data point $i$ chosen uniformly at random from $\\{1, \\ldots, m\\} .$ Show that the SGD gradient is an unbiased estimator of the real gradient $-\\nabla J(\\beta)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the linear regression from the question is: $$J(\\beta)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(\\beta^{T} x_{i}-y_{i}\\right)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\nabla J(\\beta)&=XX^T\\beta-Xy\\\\\n",
    "&=\\frac{1}{m} \\sum_{i=1}^{m}2x_ix_{i}^T\\beta-2x_{i}y_{i}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Then $$\\begin{aligned}\n",
    "\\nabla \\tilde{J}_n(\\beta)&=\\frac{1}{n} \\sum_{i=1}^{n}2x_ix_{i}^T\\beta-2x_{i}y_{i} \\text{, where $n\\le m$}\n",
    "\\end{aligned}$$\n",
    "\n",
    "And the expectation will be: \n",
    "$$\\begin{aligned}\n",
    "E[\\nabla \\tilde{J}_n(\\beta)]&=E[\\frac{1}{n} \\sum_{i=1}^{n}2x_ix^T\\beta-2x_{i}y_{i}]\\\\\n",
    "&=\\frac{1}{n} \\sum_{i=1}^{n}E[2x_ix_{i}^T\\beta-2x_{i}y_{i}]\\\\\n",
    "&=E[2x_ix_{i}^T\\beta-2x_{i}y_{i}]\\\\\n",
    "&=\\frac{1}{m} \\sum_{i=1}^{m}2x_ix_{i}^T\\beta-2x_{i}y_{i}=\\nabla J(\\beta)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, since $E[\\nabla \\tilde{J}_n(\\beta)]=\\nabla J(\\beta)$, then the SGD is an unbaised estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Derive the SGD update rule for the linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the formula on *regularization lecture notes*, \n",
    "$$\\beta^{(k+1)}:=\\beta^{(k)}+\\alpha_{k} X\\left(y-X^{T} \\beta^{(k)}\\right)\\tag{19}$$\n",
    "\n",
    "Then the SGD update rule will be:\n",
    "$$\\beta^{(k+1)}:=\\beta^{(k)}+2\\alpha_{k}\\left(y_n-x_n^{T} \\beta^{(k)}\\right)x_n \\text{ for $n=1,2,...,m$}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see (c) (d) (e) in another file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
